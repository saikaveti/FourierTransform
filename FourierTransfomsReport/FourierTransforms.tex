\documentclass{amsproc}
\usepackage{indentfirst}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{tabto}
\usepackage{fancyvrb}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphicx}


\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother



\title{The Fourier Transform: Variations and Applications}

\author{Sailesh Kaveti}

\begin{document}

\maketitle

\section{The Fourier Transform}

\subsection{Developing an Intuition for the Fourier Transform}

\mbox{}	\\
\indent Imagine we are given we are given a bucket of paint, and asked to replicate the color of that bucket. At first, it seems easy to simply describe it nominally, calling the bucket of paint as red, yellow, blue, or any other color.  This is a valid first instinct. However, imagine you were given a purple bucket of paint. Even in this instance, most people are familiar with the color purple and we could just mix equal parts of blue and red. Imagine you were given a bucket of paint that was dark purple. We could still call this bucket “dark purple”, but we would immediately run into problems when replicating the paint color. Do we need to put the colors on a 2:1 ratio or a 3:1 ratio? In general, given a color, it can be very useful to know the colors that compose that color, as opposed to just a nominal description. Now imagine that we were given a complex wave, and that we are interested in the simpler waves that compose that wave. The Fourier Transform helps us determine a periodic wave's composite frequencies.

\subsection{The Fourier Series}

\mbox{}	\\
\indent In order to understand the Fourier Transform, it is very important to gain understand the Fourier Series and its integral and the Fourier Transform is non-discretized extension of the Fourier Series. According to Dym and McKean, the authors of \textit{Fourier Series and Integrals}, the basic idea of the Fourier Series is that any periodic function $f(t)$ can be expressed as a trigonometric sum of sines and cosines. More specifically, for the same period T for both $sine$ and $cosine$, we can claim the following:

$$
f(t) = \sum_{n = 0}^{\infty} [\hat f_+(n)\cos \Big(\frac{\pi n t}{T}\Big) + \hat f_-(n)\sin \Big(\frac{\pi n t}{T}\Big)]
$$

From this, we can derive the equations for $\hat f_+(n)$ and $\hat f_-(n)$. The equations are as follows

\begin{align*}
\hat f_+(n)  &= \frac{1}{L}\int_{-L}^{L} f(t) cos\Big(\frac{n\pi x}{L}\Big)dx \\
\hat f_-(n)  &= \frac{1}{L}\int_{-L}^{L} f(t) sin\Big(\frac{n\pi x}{L}\Big)dx
\end{align*}

Putting these two together, we can get the Fourier Series, a formula to completely put a periodic function in terms of $sine$ and $cosine$. For example, a non-analytic square wave $g(x)$, that has a value of $-h$ from $-\pi$ to $0$ and h from $0$ to $pi$ with a vertical line from $-h$ to $h$ can be modeled with the following Fourier Series:

$$
g(x) = \sum_{n = 0}^{\infty} \frac{sin((2n + 1)x)}{(2n + 1)}
$$

The intuition behind a Fourier Series is very similar to a Taylor or MacLaurin Series, where a complex function can be modeled and very closely  as an infinite sum of simpler functions.

\subsection{The Computation of the Fourier Transform}

\mbox{}	\\
\indent Similar to colors, given a complex wave, it is important to know the frequencies that compose that wave. The inverse of this can also provide us with valuable information. A wave $f(x)$ exists in real space, and its equivalent in frequency space is function $F(s)$. In short, $f(x)$ gives us a complex wave in real space, while $F(s)$ gives us a function with peaks at the frequencies that make up $f(x)$.

As described in \textit{The Fourier Transforms and Its Applications}, the following relationship between F(s) and f(x) exists:

\begin{align*}
\intertext{The Fourier Transform:}
F(s)  &= \int_{-\infty}^{\infty} f(x) e^{-i2 \pi x s}dx
\intertext{The Inverse Fourier Transform:}
f(x)  &= \int_{-\infty}^{\infty} F(s) e^{i2 \pi x s}ds
\end{align*}

Replacing any instances of $e^{i\theta}$ with $cos(\theta) + isin(\theta)$, we can derive the following equivalent expressions:

\begin{align*}
\intertext{The Fourier Transform:}
F(s)  &= \int_{-\infty}^{\infty} f(x) (cos(-2 \pi x s) + isin(-2 \pi x s))dx
\intertext{The Inverse Fourier Transform:}
f(x)  &= \int_{-\infty}^{\infty} F(s) (cos(2 \pi x s) + isin(2 \pi x s))ds
\end{align*}

As a result, we can see a resemblance of a non-discretized version of the Fourier Series, which we can discretize for more interesting results in the following section.

\section{The Discrete Fourier Transform}

\subsection{The Basics of the Discrete Fourier Transform}

\subsection{How the Discrete Fourier Transform varies from the Fourier Transform}

\section{The Computation of the Discrete Fourier Transform}

\subsection{A Rudimentary Algorithm for the Discrete Fourier Transform}

\subsection{The Fast Fourier Tranform}

\mbox{}	 \\
\indent Before starting the Fast Fourier Transform (FFT), it is important to introduce a concept known as complex roots of unity. A complex $n^{th}$ root of unity is a complex number $\omega$ such that $\omega^n = 1$. As the name suggests, there are a total of $n$ distinct complex $n^{th}$ roots of unity. In order to find all $n$ complex roots of unity, we say $\omega_k = e^{\frac{2 \pi i k}{n}}, \forall k \in \{0, 1, 2, ... , n-1\}$. When $k = 0$, we yield the trivial solution of $\omega = 1$.

	Given the Discrete Fourier Transform, it seems in our interest to find a way to compute this value. In \textit{Introduction to Algorithms}, Cormen, Leiserson, Rivest, and Stein introduce the FFT which they claim takes advantage of of the special properties of the complex roots of unity to compute $DFT_n(a)$ in $O(nlog(n))$. For the sake of this implementation, they assume at $n$ is a power of two, mostly because the existing strategies for dealing with the case where $n$ is not a power of two are quite complex. As the runtime of the algorithm suggests, the FFT is a divide and conquer algorithm. Given an $A(x)$, we can represent it as an array $a$ of coefficients. In addition, we must also define the following intermediary functions that will be called in the algorithm:
	
\begin{align*}
A^{[0]}(x) &= a_0 + a_2x + a_4x^2 + ... + a_{n-2}x^{n/2 - 1} \\
A^{[1]}(x) &= a_1 + a_3x + a_5x^2 + ... + a_{n-1}x^{n/2 - 1} \\
\intertext{such that}
A(x) &= A^{[0]}(x^2) + xA^{[1]}(x^2)
\end{align*}

The following algorithm to find the FFT of $a$ is defined recursively by Cormen and is as follows:

\begin{algorithm}
\caption{Recursive Implementation of FFT}\label{fft}
\begin{algorithmic}[1]
\Procedure{Recursive-FFT}{a} \Comment{A Recursive Implementation of FFT}
\State $n = a.length$
\If {$n == 1$} \Return{ a} \Comment{Base Case of Recursion}
\EndIf
\State $\omega_n = e^{2\pi i / n}$ 
\State $\omega = 1$
\State $a^{[0]} = (a_0, a_2, a_4, ..., a_{n-2})$ \Comment{Define Coefficients}
\State $a^{[1]} = (a_1, a_3, a_5, ..., a_{n-1})$
\State $y^{[0]} = \Call{Recursive-FFT}{a^{[0]}}$ \Comment{Recursive Step}
\State $y^{[1]} = \Call{Recursive-FFT}{a^{[1]}}$

\For{ \texttt{$k = 0$ to $n/2 - 1$}}
	\State $y_k = y_k^{[0]} + \omega y_k^{[1]}$ \Comment{These hold true by definition of $y_k$}
	\State $y_{k+(n/2)} = y_k^{[0]} - \omega y_k^{[1]}$
	\State $\omega = \omega \omega_n$ \Comment{Making sure that $\omega$ is updated properly}
\EndFor

\State \Return{y} \Comment{Assumed that $y$ is a column vector}

\EndProcedure
\end{algorithmic}
\end{algorithm}

From this algorithm, we can arrive at the \textbf{Convolution Theorem}, which states that for any two vectors $a$ and $b$, both of which have a length $n$ that is a power of two, the following holds true:

$$
a \otimes b = DFT_{2n}^{-1}(DFT_{2n}(a) DFT_{2n}(b))
$$

\subsection{Alternatives for the Fast Fourier Tranform}

\mbox{}	 \\
\indent In \textit{Introduction to Algorithms}, Cormen, Leiserson, Rivest, and Stein present certain improvements to existing implementation of the FFT. One notable improvement is an iterative implementation of the FFT. In Computer Science, iterative and recursive solutions are often considered equivalent. This is proved in the Church-Turing Thesis. However, there certainly runtime tradeoffs that we sometimes have to make when converting from recursion to iteration and vice-versa. That being said, we make no computational trade-offs in this instance, as our iterative solution still has a runtime of $O(nlog(n))$. In this instance, just like the recursive implementation, $a$ must have a length that is a power of 2. The algorithm for an iterative approach to the FFT is as follows:

\begin{algorithm}
\caption{Iterative Implementation of FFT}\label{it-fft}
\begin{algorithmic}[1]
\Procedure{Bit-Reverse-Copy}{a, A} 

\State $n = a.length$
\For {\texttt{$k = 0$ to $n-1$}}
	\State $A[rev(k)] = a_k$ \Comment{$rev(k)$ is the reverse of the bit-wise representation of k}
\EndFor

\EndProcedure

\Procedure{Iterative-FFT}{a}

\State \Call{Bit-Reverse-Copy}{a, A}
\For {\texttt{$s = 1$ to $log_2(n)$}}
	\State $m = 2^s$
	\State $\omega_m = e^{2 \pi i / m}$
	\For {\texttt{$k = 0$ to $n - 1$ by $m$}}
		\State $w = 1$
		\For {\texttt{$j = 0$ to $m/2 - 1$}}
			\State $t = \omega A[k + j + m/2]$
			\State $u = A[k+j]$
			\State $A[k+j] = u + t$
			\State $A[k+j+m/2] = u - t$
			\State $\omega = \omega \omega_m$
		\EndFor
	\EndFor
\EndFor

\EndProcedure

\end{algorithmic}
\end{algorithm}

\subsection{A Python Implementation of the Fast Fourier Transform}

In order to better understand the results of the Fast Fourier Transform, it may be useful to write code to explore a Pythonic implementation, and  plot results in order to visualize the results of the Fast Fourier Transform. There are existing libraries in Python to aid with the visualization of the Fast Fourier Transformation. These include numpy and matplotlib. Numpy is useful for existing libraries that make it possible to manipulate an entire array with a single operation as well as contains the function to perform the Fast Fourier Transformation. Matplotlib is the library that contains the function to fully plot a dataset. Given a signal, the following method plots both the signal in real space as well as in frequency space:

\begin{Verbatim}[tabsize=4]
def plot_signal(signal):
	signal_fft = np.fft.fft(signal)
	signal_fft_pow = np.abs(signal_fft)
	plt.plot(signal, '-o', markersize = 2)
	plt.savefig('RealSpace.png')
	plt.clf()
	plt.plot(signal_fft_pow, '-o', markersize = 2)
	plt.savefig('FrequencySpace.png')
\end{Verbatim}

With that information, we are able to quickly find the FFT of different waves in real space in frequency space. Here are a few examples generated as a result of the python code included above. 

\begin{figure}[h]
\centering
\begin{subfigure}{.4\textwidth}
	\centering
	\includegraphics[scale=.3]{FFTOriginal1.png}
	\caption{Real Space}
	\label{fig:sub1}
\end{subfigure} %
\begin{subfigure}{.4\textwidth}
	\centering
	\includegraphics[scale=.3]{FFTResult1.png}
	\caption{Frequency Space}
	\label{fig:sub2}
\end{subfigure}
\caption{Wave 1 and its Fast Fourier Transform}
\label{fig:text1}
\end{figure}


\begin{figure}[h]
\centering
\begin{subfigure}{.4\textwidth}
	\centering
	\includegraphics[scale=.3]{FFTOriginal2.png}
	\caption{Real Space}
	\label{fig:sub3}
\end{subfigure} %
\begin{subfigure}{.4\textwidth}
	\centering
	\includegraphics[scale=.3]{FFTResult2.png}
	\caption{Frequency Space}
	\label{fig:sub4}
\end{subfigure}
\caption{Wave 2 and its Fast Fourier Transform}
\label{fig:text2}
\end{figure}
\section{Applications of the Fourier Transform}

\subsection{Multiplication of Polynomials}

\mbox{}	 \\
\indent According to \textit{Introduction to Algorithms}, the straightforward method of adding two polynomials takes $O(n)$ time. We know the following to hold true:

\begin{align}
f(x) &= \sum_{j = 0}^{n} a_j x^j \\
g(x) &= \sum_{j = 0}^{n} b_j x^j \\ 
f(x) + g(x) &= \sum_{j = 0}^{n} (a_j + b_j)  x^j 
\end{align}

For two polynomials $f(x)$ and $g(x)$, with length $k$ and $n$, we know that $f(x) + g(x)$ has length $max(k, n)$. Generate two arrays of length $n$ called $A_f$ and $A_g$ where each index $i$ represents the coefficient for term $x^i$ for $f(x)$ and $g(x)$ respectively. Iterate through the arrays and add the value held at each index of $A_f[i]$ and $A_g[i]$ and set that value in $A_{f+g}[i]$. The resulting array represents the coefficients of $f(x)+g(x)$.

With this in mind, we can derive the straightforward algorithm for the multiplication of two polynomials. For the multiplication of the two polynomials, Cormen claims that the following holds true:

\begin{align}
f(x)g(x) &= \sum_{j = 0}^{2n} c_j  x^j \\
\intertext{where}
c_j &=  \sum_{k = 0}^{j} a_k b_{j-k}
\end{align}

From this information, we can see that equation $(4)$ yields us with the sum of $2n$ polynomials, each of which was computed in $O(n)$ time. As we showed earlier, the sum of any two polynomials can be computed in $O(n)$ time, and since we must add $2n$ total polynomials, we get that the product of any two polynomials can be computed in $O(2n^2)$ time, which can be reduced to $O(n^2)$ time by the definition of big-$O$ notation.

With our understanding of the FFT, Cormen claims that we can find the product of any two polynomials in $O(nlog(n))$ time, a significant asymptotic improvement over $O(n^2)$.  Before starting the explanation of the algorithm, it is important to introduce another way to represent a polynomial, known as the point-value representation. 

The point-value representation of a polynomial $A(x)$ is a set of pairs in the form ${\{(x_0, y_0), (x_1, y_1), ..... , (x_n, y_n)\}}$ such that $y_i = A(x_i)$.  These values can be generated in $O(n^2)$  time using Horner's Metho, but can be reduced to $O(nlog(n))$ time if the values of $x$ are cleverly picked. From this definition, one can conclude that each polynomial has a many different point-value representations. Using this new representation of polynomials, we can indeed determine the product of any two polynomials in $O(n)$ time. Given this information, we can now explore the algorithm to find the product of any two polynomials in $O(nlog(n))$ time. Cormen argues that the following four steps will determine the product of any two polynomials $A(x)$ and $B(x)$ of length n:

\begin{enumerate}
	\item \textit{Double degree-bound: } Create the coefficient representations of $A(x)$ and $B(x)$ of length $2n$ by padding $n$ higher-order coefficients to the left of the array representations of $A(x)$ and $B(x)$. This can be done in $O(n)$ time.
	\item \textit{Evaluate: } Compute the point-value representations of $A(x)$ and $B(x)$ of length $2n$ by applying the FFT of order $2n$ on each polynomial. As a result, we can get the representations at the $(2n)^{th}$ roots of unity. This is done in $O(nlog(n))$ time.
	\item \textit{Pointwise Multiply: } Multiply these two point-value representations pairwise, yielding us with the value of $C(x) = A(x)B(x)$ at all roots of unity. As explained earlier, this is done in $O(n)$ time.
	\item \textit{Iterpolate: } We can then create the coefficient representation of polynomial  $C(x)$ by applying the FFT on the point-value representation of $C(x)$. This is done in $O(nlog(n))$ time.

\end{enumerate}

This entire process is done in $O(n + nlog(n) + n + nlog(n))$ time, which can reduce to $O(nlog(n))$ time by the definition of big-$O$ notation. Thus, using the above algorithm, one can multiply any two polynomials in $O(nlog(n))$ time.

\subsection{Image Processing}

\end{document}